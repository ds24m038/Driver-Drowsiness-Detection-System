{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training - Driver Drowsiness Detection\n",
        "\n",
        "This notebook implements the full training pipeline with **W&B Sweeps** for hyperparameter optimization:\n",
        "- Data loading and preprocessing\n",
        "- Train/validation/test splits\n",
        "- PyTorch CNN model training\n",
        "- **W&B Sweeps for automatic hyperparameter tuning**\n",
        "- Model checkpointing\n",
        "- Evaluation metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import wandb\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Add src to path\n",
        "PROJECT_ROOT = Path().resolve().parent\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "sys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv(PROJECT_ROOT / \".env\")\n",
        "\n",
        "from src.backend.models import DrowsinessCNN, create_model\n",
        "from src.config.settings import WANDB_PROJECT, WANDB_API_KEY, MODEL_INPUT_SIZE\n",
        "\n",
        "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
        "MODELS_DIR.mkdir(exist_ok=True)\n",
        "MODEL_PATH = MODELS_DIR / \"best_model.pth\"\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"MPS available: {torch.backends.mps.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## W&B Sweep Configuration\n",
        "\n",
        "Define the hyperparameter search space for automatic tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sweep configuration for hyperparameter search\n",
        "SWEEP_CONFIG = {\n",
        "    \"method\": \"bayes\",  # Options: \"grid\", \"random\", \"bayes\"\n",
        "    \"metric\": {\n",
        "        \"name\": \"val_accuracy\",\n",
        "        \"goal\": \"maximize\"\n",
        "    },\n",
        "    \"parameters\": {\n",
        "        \"learning_rate\": {\n",
        "            \"distribution\": \"log_uniform_values\",\n",
        "            \"min\": 0.0001,\n",
        "            \"max\": 0.01,\n",
        "        },\n",
        "        \"batch_size\": {\n",
        "            \"values\": [16, 32, 64]\n",
        "        },\n",
        "        \"epochs\": {\n",
        "            \"values\": [5, 10, 15]\n",
        "        },\n",
        "        \"dropout_rate\": {\n",
        "            \"values\": [0.3, 0.5, 0.7]\n",
        "        },\n",
        "        \"optimizer\": {\n",
        "            \"values\": [\"Adam\", \"SGD\", \"AdamW\"]\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "print(\"Sweep Configuration:\")\n",
        "print(f\"  Method: {SWEEP_CONFIG['method']}\")\n",
        "print(f\"  Metric: {SWEEP_CONFIG['metric']}\")\n",
        "print(f\"  Parameters: {list(SWEEP_CONFIG['parameters'].keys())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset class\n",
        "class DrowsinessDataset(Dataset):\n",
        "    def __init__(self, drowsy_dir, non_drowsy_dir, transform=None):\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        \n",
        "        # Load Drowsy images (label 1)\n",
        "        drowsy_path = Path(drowsy_dir)\n",
        "        for img_path in drowsy_path.glob(\"*.png\"):\n",
        "            self.images.append(str(img_path))\n",
        "            self.labels.append(1)\n",
        "        \n",
        "        # Load Non Drowsy images (label 0)\n",
        "        non_drowsy_path = Path(non_drowsy_dir)\n",
        "        for img_path in non_drowsy_path.glob(\"*.png\"):\n",
        "            self.images.append(str(img_path))\n",
        "            self.labels.append(0)\n",
        "        \n",
        "        print(f\"Loaded {len(self.images)} images\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "\n",
        "# Data transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(MODEL_INPUT_SIZE),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(MODEL_INPUT_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_device():\n",
        "    \"\"\"Get the best available device.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "def create_data_loaders(batch_size):\n",
        "    \"\"\"Create data loaders with proper transform separation.\"\"\"\n",
        "    DATA_DIR = PROJECT_ROOT / \"Data\"\n",
        "    drowsy_dir = DATA_DIR / \"Drowsy\"\n",
        "    non_drowsy_dir = DATA_DIR / \"Non Drowsy\"\n",
        "    \n",
        "    # Create separate datasets for different transforms\n",
        "    train_full = DrowsinessDataset(drowsy_dir, non_drowsy_dir, transform=train_transform)\n",
        "    val_full = DrowsinessDataset(drowsy_dir, non_drowsy_dir, transform=val_transform)\n",
        "    \n",
        "    # Split indices\n",
        "    total_size = len(train_full)\n",
        "    train_size = int(0.7 * total_size)\n",
        "    val_size = int(0.15 * total_size)\n",
        "    \n",
        "    generator = torch.Generator().manual_seed(42)\n",
        "    indices = torch.randperm(total_size, generator=generator).tolist()\n",
        "    \n",
        "    train_indices = indices[:train_size]\n",
        "    val_indices = indices[train_size:train_size + val_size]\n",
        "    test_indices = indices[train_size + val_size:]\n",
        "    \n",
        "    train_dataset = Subset(train_full, train_indices)\n",
        "    val_dataset = Subset(val_full, val_indices)\n",
        "    test_dataset = Subset(val_full, test_indices)\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "    \n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "def get_optimizer(model, name, lr):\n",
        "    \"\"\"Get optimizer by name.\"\"\"\n",
        "    if name == \"Adam\":\n",
        "        return optim.Adam(model.parameters(), lr=lr)\n",
        "    elif name == \"AdamW\":\n",
        "        return optim.AdamW(model.parameters(), lr=lr)\n",
        "    elif name == \"SGD\":\n",
        "        return optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    return optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    return running_loss / len(train_loader), 100 * correct / total\n",
        "\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate the model.\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds, all_labels = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_preds, average=\"weighted\", zero_division=0\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        \"loss\": running_loss / len(val_loader),\n",
        "        \"accuracy\": 100 * correct / total,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(config=None, save_best=True):\n",
        "    \"\"\"Training function for sweeps.\"\"\"\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "        \n",
        "        # Get hyperparameters\n",
        "        lr = config.get(\"learning_rate\", 0.001)\n",
        "        batch_size = config.get(\"batch_size\", 32)\n",
        "        epochs = config.get(\"epochs\", 10)\n",
        "        dropout_rate = config.get(\"dropout_rate\", 0.5)\n",
        "        optimizer_name = config.get(\"optimizer\", \"Adam\")\n",
        "        \n",
        "        print(f\"\\nTraining: lr={lr:.6f}, batch={batch_size}, epochs={epochs}, dropout={dropout_rate}, opt={optimizer_name}\")\n",
        "        \n",
        "        device = get_device()\n",
        "        train_loader, val_loader, test_loader = create_data_loaders(batch_size)\n",
        "        \n",
        "        # Create model with dropout rate\n",
        "        model = create_model(num_classes=2)\n",
        "        for module in model.modules():\n",
        "            if isinstance(module, nn.Dropout):\n",
        "                module.p = dropout_rate\n",
        "        model = model.to(device)\n",
        "        \n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = get_optimizer(model, optimizer_name, lr)\n",
        "        \n",
        "        best_val_acc = 0.0\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "            val_metrics = validate(model, val_loader, criterion, device)\n",
        "            \n",
        "            wandb.log({\n",
        "                \"epoch\": epoch,\n",
        "                \"train_loss\": train_loss,\n",
        "                \"train_accuracy\": train_acc,\n",
        "                \"val_loss\": val_metrics[\"loss\"],\n",
        "                \"val_accuracy\": val_metrics[\"accuracy\"],\n",
        "                \"val_f1\": val_metrics[\"f1\"],\n",
        "            })\n",
        "            \n",
        "            print(f\"  Epoch {epoch+1}/{epochs}: val_acc={val_metrics['accuracy']:.2f}%\")\n",
        "            \n",
        "            if val_metrics[\"accuracy\"] > best_val_acc:\n",
        "                best_val_acc = val_metrics[\"accuracy\"]\n",
        "                if save_best:\n",
        "                    torch.save({\n",
        "                        \"model_state_dict\": model.state_dict(),\n",
        "                        \"config\": dict(config),\n",
        "                    }, MODEL_PATH)\n",
        "        \n",
        "        # Test evaluation\n",
        "        test_metrics = validate(model, test_loader, criterion, device)\n",
        "        wandb.log({\"test_accuracy\": test_metrics[\"accuracy\"], \"best_val_accuracy\": best_val_acc})\n",
        "        \n",
        "        print(f\"  Best val_acc: {best_val_acc:.2f}%, Test acc: {test_metrics['accuracy']:.2f}%\")\n",
        "        return best_val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Option 1: Run Hyperparameter Sweep\n",
        "\n",
        "This will automatically search for the best hyperparameters using Bayesian optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Login to W&B\n",
        "wandb.login(key=WANDB_API_KEY)\n",
        "\n",
        "# Number of sweep trials\n",
        "SWEEP_COUNT = 10  # Adjust as needed\n",
        "\n",
        "# Create and run sweep\n",
        "sweep_id = wandb.sweep(SWEEP_CONFIG, project=WANDB_PROJECT)\n",
        "print(f\"Sweep ID: {sweep_id}\")\n",
        "print(f\"Running {SWEEP_COUNT} trials...\")\n",
        "\n",
        "wandb.agent(sweep_id, function=lambda: train_model(save_best=False), count=SWEEP_COUNT)\n",
        "\n",
        "print(f\"\\n✓ Sweep complete! View results at: https://wandb.ai/{WANDB_PROJECT}/sweeps/{sweep_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Option 2: Train with Specific Configuration\n",
        "\n",
        "Use this to train with a specific set of hyperparameters (e.g., the best ones from the sweep)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train with specific config and save model\n",
        "best_config = {\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"batch_size\": 32,\n",
        "    \"epochs\": 10,\n",
        "    \"dropout_rate\": 0.5,\n",
        "    \"optimizer\": \"Adam\",\n",
        "}\n",
        "\n",
        "wandb.login(key=WANDB_API_KEY)\n",
        "\n",
        "with wandb.init(project=WANDB_PROJECT, name=\"best_model_training\", config=best_config):\n",
        "    best_val_acc = train_model(config=wandb.config, save_best=True)\n",
        "    \n",
        "    # Save as W&B artifact\n",
        "    artifact = wandb.Artifact(\"drowsiness_detection_model\", type=\"model\")\n",
        "    artifact.add_file(str(MODEL_PATH))\n",
        "    wandb.log_artifact(artifact)\n",
        "    print(f\"\\n✓ Model saved to W&B as artifact\")\n",
        "\n",
        "print(f\"\\n✓ Training complete! Model saved to {MODEL_PATH}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}