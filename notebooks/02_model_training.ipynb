{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training - Driver Drowsiness Detection CNN\n",
        "\n",
        "This notebook trains a PyTorch CNN model for binary classification of driver drowsiness.\n",
        "\n",
        "## Steps:\n",
        "1. Load and prepare the dataset\n",
        "2. Define the CNN architecture\n",
        "3. Set up training loop with W&B logging\n",
        "4. Train the model\n",
        "5. Evaluate on test set\n",
        "6. Save the best model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import wandb\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "from src.models.cnn_model import DrowsinessCNN\n",
        "from src.config.settings import WANDB_PROJECT, WANDB_API_KEY\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DrowsinessDataset(Dataset):\n",
        "    \"\"\"Dataset class for driver drowsiness images.\"\"\"\n",
        "    \n",
        "    def __init__(self, drowsy_dir, non_drowsy_dir, transform=None):\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        \n",
        "        # Load drowsy images (label 1)\n",
        "        drowsy_path = Path(drowsy_dir)\n",
        "        for img_path in drowsy_path.glob(\"*.png\"):\n",
        "            self.images.append(str(img_path))\n",
        "            self.labels.append(1)  # drowsy = 1\n",
        "        \n",
        "        # Load non-drowsy images (label 0)\n",
        "        non_drowsy_path = Path(non_drowsy_dir)\n",
        "        for img_path in non_drowsy_path.glob(\"*.png\"):\n",
        "            self.images.append(str(img_path))\n",
        "            self.labels.append(0)  # alert = 0\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        # Load image\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        \n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Split Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define paths\n",
        "data_dir = project_root / \"Data\"\n",
        "drowsy_dir = data_dir / \"Drowsy\"\n",
        "non_drowsy_dir = data_dir / \"Non Drowsy\"\n",
        "\n",
        "# Define transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((227, 227)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((227, 227)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create full dataset\n",
        "full_dataset = DrowsinessDataset(drowsy_dir, non_drowsy_dir, transform=None)\n",
        "print(f\"Total images: {len(full_dataset)}\")\n",
        "\n",
        "# Split: 70% train, 15% val, 15% test\n",
        "train_size = int(0.7 * len(full_dataset))\n",
        "val_size = int(0.15 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    full_dataset, [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "# Apply transforms\n",
        "train_dataset.dataset.transform = train_transform\n",
        "val_dataset.dataset.transform = val_test_transform\n",
        "test_dataset.dataset.transform = val_test_transform\n",
        "\n",
        "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize W&B\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize W&B\n",
        "wandb.login(key=WANDB_API_KEY)\n",
        "\n",
        "wandb.init(\n",
        "    project=WANDB_PROJECT,\n",
        "    config={\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": 10,\n",
        "        \"optimizer\": \"Adam\",\n",
        "        \"model_architecture\": \"DrowsinessCNN\",\n",
        "        \"input_size\": \"227x227\",\n",
        "        \"num_classes\": 2,\n",
        "        \"train_size\": len(train_dataset),\n",
        "        \"val_size\": len(val_dataset),\n",
        "        \"test_size\": len(test_dataset),\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Initialize Model, Loss, and Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = DrowsinessCNN(num_classes=2).to(device)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    epoch_loss = running_loss / len(loader)\n",
        "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
        "    epoch_prec = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    epoch_rec = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    epoch_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    \n",
        "    return epoch_loss, epoch_acc, epoch_prec, epoch_rec, epoch_f1\n",
        "\n",
        "\n",
        "def validate(model, loader, criterion, device):\n",
        "    \"\"\"Validate the model.\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    epoch_loss = running_loss / len(loader)\n",
        "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
        "    epoch_prec = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    epoch_rec = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    epoch_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    \n",
        "    return epoch_loss, epoch_acc, epoch_prec, epoch_rec, epoch_f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training\n",
        "num_epochs = wandb.config.epochs\n",
        "best_val_acc = 0.0\n",
        "best_model_path = project_root / \"models\" / \"best_model.pth\"\n",
        "\n",
        "# Create models directory\n",
        "best_model_path.parent.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    # Train\n",
        "    train_loss, train_acc, train_prec, train_rec, train_f1 = train_epoch(\n",
        "        model, train_loader, criterion, optimizer, device\n",
        "    )\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_acc, val_prec, val_rec, val_f1 = validate(\n",
        "        model, val_loader, criterion, device\n",
        "    )\n",
        "    \n",
        "    # Update learning rate\n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    # Log to W&B\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": train_loss,\n",
        "        \"train_accuracy\": train_acc,\n",
        "        \"train_precision\": train_prec,\n",
        "        \"train_recall\": train_rec,\n",
        "        \"train_f1\": train_f1,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"val_accuracy\": val_acc,\n",
        "        \"val_precision\": val_prec,\n",
        "        \"val_recall\": val_rec,\n",
        "        \"val_f1\": val_f1,\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "    })\n",
        "    \n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "    print(f\"  Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}\")\n",
        "    print(f\"  Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
        "    \n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "            'val_loss': val_loss,\n",
        "        }, best_model_path)\n",
        "        print(f\"  âœ“ Saved best model (val_acc: {val_acc:.4f})\")\n",
        "        \n",
        "        # Log model artifact to W&B\n",
        "        artifact = wandb.Artifact('best_model', type='model')\n",
        "        artifact.add_file(str(best_model_path))\n",
        "        wandb.log_artifact(artifact)\n",
        "\n",
        "print(f\"\\nTraining complete! Best validation accuracy: {best_val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test Set Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model\n",
        "checkpoint = torch.load(best_model_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(\"Loaded best model for testing\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc, test_prec, test_rec, test_f1 = validate(model, test_loader, criterion, device)\n",
        "\n",
        "# Confusion matrix\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Log test metrics to W&B\n",
        "wandb.log({\n",
        "    \"test_loss\": test_loss,\n",
        "    \"test_accuracy\": test_acc,\n",
        "    \"test_precision\": test_prec,\n",
        "    \"test_recall\": test_rec,\n",
        "    \"test_f1\": test_f1\n",
        "})\n",
        "\n",
        "print(f\"\\nTest Results:\")\n",
        "print(f\"  Loss: {test_loss:.4f}\")\n",
        "print(f\"  Accuracy: {test_acc:.4f}\")\n",
        "print(f\"  Precision: {test_prec:.4f}\")\n",
        "print(f\"  Recall: {test_rec:.4f}\")\n",
        "print(f\"  F1-Score: {test_f1:.4f}\")\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Alert', 'Drowsy'], \n",
        "            yticklabels=['Alert', 'Drowsy'])\n",
        "plt.title('Confusion Matrix - Test Set')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Log confusion matrix to W&B\n",
        "wandb.log({\"confusion_matrix\": wandb.Image(plt)})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Finish W&B Run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.finish()\n",
        "print(\"W&B run completed!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
