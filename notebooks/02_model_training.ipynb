{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training - Driver Drowsiness Detection\n",
        "\n",
        "This notebook implements the full training pipeline for the driver drowsiness detection model:\n",
        "- Data loading and preprocessing\n",
        "- Train/validation/test splits\n",
        "- PyTorch CNN model training\n",
        "- Weights & Biases (W&B) integration for experiment tracking\n",
        "- Model checkpointing\n",
        "- Evaluation metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "import wandb\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Add src to path\n",
        "PROJECT_ROOT = Path().resolve().parent\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "sys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv(PROJECT_ROOT / \".env\")\n",
        "\n",
        "from src.backend.models import DrowsinessCNN, create_model\n",
        "from src.config.settings import WANDB_PROJECT, WANDB_API_KEY, MODEL_PATH, MODEL_INPUT_SIZE\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize W&B\n",
        "wandb.login(key=WANDB_API_KEY)\n",
        "wandb.init(\n",
        "    project=WANDB_PROJECT,\n",
        "    name=\"drowsiness_cnn_training\",\n",
        "    config={\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 20,\n",
        "        \"model_architecture\": \"DrowsinessCNN\",\n",
        "        \"input_size\": MODEL_INPUT_SIZE,\n",
        "        \"num_classes\": 2,\n",
        "        \"optimizer\": \"Adam\",\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset class\n",
        "class DrowsinessDataset(Dataset):\n",
        "    def __init__(self, drowsy_dir, non_drowsy_dir, transform=None):\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        \n",
        "        # Load Drowsy images (label 1)\n",
        "        drowsy_path = Path(drowsy_dir)\n",
        "        for img_path in drowsy_path.glob(\"*.png\"):\n",
        "            self.images.append(str(img_path))\n",
        "            self.labels.append(1)  # drowsy = 1\n",
        "        \n",
        "        # Load Non Drowsy images (label 0)\n",
        "        non_drowsy_path = Path(non_drowsy_dir)\n",
        "        for img_path in non_drowsy_path.glob(\"*.png\"):\n",
        "            self.images.append(str(img_path))\n",
        "            self.labels.append(0)  # alert = 0\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        # Load image\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, label\n",
        "\n",
        "# Data transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(MODEL_INPUT_SIZE),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(MODEL_INPUT_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "DATA_DIR = PROJECT_ROOT / \"Data\"\n",
        "drowsy_dir = DATA_DIR / \"Drowsy\"\n",
        "non_drowsy_dir = DATA_DIR / \"Non Drowsy\"\n",
        "\n",
        "full_dataset = DrowsinessDataset(drowsy_dir, non_drowsy_dir, transform=None)\n",
        "\n",
        "# Split dataset: 70% train, 15% val, 15% test\n",
        "train_size = int(0.7 * len(full_dataset))\n",
        "val_size = int(0.15 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    full_dataset, [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "# Apply transforms\n",
        "train_dataset.dataset.transform = train_transform\n",
        "val_dataset.dataset.transform = val_transform\n",
        "test_dataset.dataset.transform = val_transform\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = wandb.config.batch_size\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = create_model(num_classes=2)\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
        "\n",
        "print(f\"Model created on device: {device}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# Validation function\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    \n",
        "    # Calculate precision, recall, F1\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_preds, average=\"weighted\", zero_division=0\n",
        "    )\n",
        "    \n",
        "    return epoch_loss, epoch_acc, precision, recall, f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "epochs = wandb.config.epochs\n",
        "best_val_acc = 0.0\n",
        "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
        "MODELS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_acc, precision, recall, f1 = validate(model, val_loader, criterion, device)\n",
        "    \n",
        "    # Log to W&B\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch,\n",
        "        \"train_loss\": train_loss,\n",
        "        \"train_accuracy\": train_acc,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"val_accuracy\": val_acc,\n",
        "        \"val_precision\": precision,\n",
        "        \"val_recall\": recall,\n",
        "        \"val_f1\": f1,\n",
        "    })\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "    print(f\"  Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
        "    \n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        model_path = MODELS_DIR / \"best_model.pth\"\n",
        "        torch.save({\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"epoch\": epoch,\n",
        "            \"val_acc\": val_acc,\n",
        "        }, model_path)\n",
        "        print(f\"  ✓ Saved best model (val_acc: {val_acc:.2f}%)\")\n",
        "        \n",
        "        # Log model artifact to W&B\n",
        "        artifact = wandb.Artifact(\"best_model\", type=\"model\")\n",
        "        artifact.add_file(str(model_path))\n",
        "        wandb.log_artifact(artifact)\n",
        "\n",
        "print(f\"\\nTraining completed! Best validation accuracy: {best_val_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model and evaluate on test set\n",
        "model.load_state_dict(torch.load(MODELS_DIR / \"best_model.pth\")[\"model_state_dict\"])\n",
        "test_loss, test_acc, test_precision, test_recall, test_f1 = validate(model, test_loader, criterion, device)\n",
        "\n",
        "print(f\"Test Results:\")\n",
        "print(f\"  Loss: {test_loss:.4f}\")\n",
        "print(f\"  Accuracy: {test_acc:.2f}%\")\n",
        "print(f\"  Precision: {test_precision:.4f}\")\n",
        "print(f\"  Recall: {test_recall:.4f}\")\n",
        "print(f\"  F1 Score: {test_f1:.4f}\")\n",
        "\n",
        "# Log test metrics to W&B\n",
        "wandb.log({\n",
        "    \"test_loss\": test_loss,\n",
        "    \"test_accuracy\": test_acc,\n",
        "    \"test_precision\": test_precision,\n",
        "    \"test_recall\": test_recall,\n",
        "    \"test_f1\": test_f1,\n",
        "})\n",
        "\n",
        "wandb.finish()\n",
        "print(\"\\n✓ Training complete! Model saved and logged to W&B.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
